{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "novels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s99436q/RNN/blob/master/novels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BjXorqANZ8c",
        "colab_type": "text"
      },
      "source": [
        "# Set Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65whMUhDO-p0",
        "colab_type": "text"
      },
      "source": [
        "###Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYk9h4M0R_Z1",
        "colab_type": "code",
        "outputId": "17ea3faa-dfb1-4319-8dd6-d42ad34d139f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "text=open(\"novel2.txt\", encoding=\"utf-8\").read()\n",
        "print(f\"天龍八部小說共有 {len(text)} 中文字,{len(set(text))} 個獨一無二的字\")\n",
        "print(text[100031:100099])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "2.2.0-rc4\n",
            "rmdir: failed to remove 'training_checkpoints/': Directory not empty\n",
            "天龍八部小說共有 1266445 中文字,4401 個獨一無二的字\n",
            "凶神惡煞’南海鱷神之上，必定是個狠惡可怖之極的人物，那知居然頗有姿色，不由得又向她瞧了幾眼。葉二孃向她嫣然一笑，木婉清全身一顫，只覺她這\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjxchXLENZ83",
        "colab_type": "text"
      },
      "source": [
        "### Prepare data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcp3TTryNZ86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#intial tokenizor \n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=None,char_level=True,filters='')\n",
        "#read the text\n",
        "tokenizer.fit_on_texts(text)\n",
        "#transfer text to int\n",
        "text_as_int = tokenizer.texts_to_sequences([text])[0] #return text=[[...]]\n",
        "#transfer text_as_int to tensor\n",
        "text = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "#make mappinf between char and index\n",
        "\n",
        "idx2char=tokenizer.index_word\n",
        "char2idx=tokenizer.word_index\n",
        "\n",
        "# each seq to input/output pairs\n",
        "def build_pairs(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "def dataset(batch_size,seq_len,steps_per_epoch):\n",
        "    #divide the novel text to small pieces of seqs\n",
        "    seqs = text.batch(seq_len + 1,drop_remainder=True)\n",
        "    #generate input/output pairs for each seq, shffle them, batch them\n",
        "    ds=seqs.map(build_pairs).batch(batch_size,drop_remainder=True)\n",
        "    rds=seqs.map(build_pairs).shuffle(steps_per_epoch).batch(batch_size,drop_remainder=True)\n",
        "    return ds,rds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCQck3zCNZ9B",
        "colab_type": "text"
      },
      "source": [
        "# Set Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzKVS0WOUiz",
        "colab_type": "text"
      },
      "source": [
        "### Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "K4ZkLAB1NZ9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(embedding_dim,lstm_units,vocab_size,batch_size):\n",
        "      #Make a LSTM model\n",
        "      model = tf.keras.Sequential()\n",
        "      #Word embedding: map a chinese char to high dimension vectors\n",
        "      model.add(\n",
        "        tf.keras.layers.Embedding(\n",
        "          input_dim=vocab_size, \n",
        "          output_dim=embedding_dim,\n",
        "          batch_input_shape=[batch_size, None]\n",
        "        )\n",
        "      )\n",
        "\n",
        "      #LSTM: process the sequence of data\n",
        "      model.add(\n",
        "        tf.keras.layers.LSTM(\n",
        "          units=lstm_units, \n",
        "          return_sequences=True, \n",
        "          stateful=True, \n",
        "          recurrent_initializer='glorot_uniform'\n",
        "        )\n",
        "      )\n",
        "\n",
        "      #Dense layer: predict the probability of each chinese char\n",
        "      model.add(\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "      )\n",
        "\n",
        "      model.summary()\n",
        "      return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4deo5Q9NZ9K",
        "colab_type": "text"
      },
      "source": [
        "### Compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtHVrPMtNZ9L",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#loss function and optimizer\n",
        "def loss(y_true, y_pred):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
        "          y_true, y_pred, from_logits=True\n",
        "    ) #becase our model return logits, so set from_logits flag\n",
        "\n",
        "def compile_model(model,learning_rate):\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
        "        loss=loss\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5ifWGcmyziq",
        "colab_type": "text"
      },
      "source": [
        "###Set check points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI7WHkv5bNXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_checkpoints(lr,seq_len,batch_size,embedding_dim,lstm_units):\n",
        "  \n",
        "    model_id=f'{lr}seq{seq_len}batch{batch_size}_embed{embedding_dim}unit{lstm_units}'\n",
        "  \n",
        "    #checkpoint_dir = './training_checkpoints/' + model_id \n",
        "    #checkpoint_prefix = checkpoint_dir + '/ckpt_{epoch}'\n",
        "    checkpoint_filepath='./training_checkpoints/' + model_id \n",
        "    log_dir = \"logs/fit/\" + model_id\n",
        "    \n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
        "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "              save_weights_only=True,save_best_only=True,\n",
        "              monitor='vcc_loss',mode='min'),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3),\n",
        "    ]\n",
        "\n",
        "    return checkpoint_filepath,callbacks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrUL6b6a6jCz",
        "colab_type": "text"
      },
      "source": [
        "### Fit model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpdkGTGM7VbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(x,epochs,callbacks): model.fit(x=x,epochs=epochs,callbacks=[callbacks])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utUCWEJuNZ9T",
        "colab_type": "text"
      },
      "source": [
        "#Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVBmLAXD70Tc",
        "colab_type": "text"
      },
      "source": [
        "### Set parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PxiBEesHbwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=0.001\n",
        "seq_len=100\n",
        "batch_size=128\n",
        "embedding_dim=512\n",
        "lstm_units=1024\n",
        "epochs=3\n",
        "vocab_size=len(set(text_as_int))\n",
        "steps_per_epoch=len(text_as_int)//seq_len\n",
        "data_chuck=0 #0 is non-shuffled data and 1 is shffled data\n",
        "temperature=0.6\n",
        "num_generate=150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwLRewk_-8fq",
        "colab_type": "text"
      },
      "source": [
        "### Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE8qk7Yd_CnE",
        "colab_type": "code",
        "outputId": "7f4c300f-a775-4de4-fa75-83d2dc0d68f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "model=build_model(embedding_dim,lstm_units,vocab_size,batch_size)\n",
        "compile_model(model,lr)\n",
        "data=dataset(batch_size,seq_len,steps_per_epoch)\n",
        "checkpoint_filepath,callbacks=set_checkpoints(lr, seq_len, batch_size,embedding_dim,lstm_units)\n",
        "fit_model(data[data_chuck],epochs,callbacks)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3f71125726f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheckpoint_filepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_chuck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'build_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZfU1J8MNZ9b",
        "colab_type": "text"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3GGfj6-iJRr",
        "colab_type": "text"
      },
      "source": [
        "###Retore the lastes Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9xBYrH2iA7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjzy2OtqlRUH",
        "colab_type": "text"
      },
      "source": [
        "###Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDm7wxblQdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model,temperature,start_string,num_generate):\n",
        "  \n",
        "  input_val=[char2idx[c] for c in start_string] #convert string to nums (vectorizing)\n",
        "  input_val=tf.expand_dims(input_val,0)\n",
        "  \n",
        "  text_generated=[]\n",
        "  \n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predict=model(input_val)\n",
        "    predict=tf.squeeze(predict,0)\n",
        "    predict/=temperature\n",
        "    \n",
        "    predict_id=tf.random.categorical(predict,num_samples=1)[-1,0].numpy()\n",
        "    input_val=tf.expand_dims([predict_id],0) \n",
        "    text_generated+=[idx2char[predict_id]]\n",
        "\n",
        "  return start_string+''.join(text_generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R9YY4zUjBOT",
        "colab_type": "text"
      },
      "source": [
        "###Build a new Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S92OGgv6jAie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=build_model(embedding_dim,lstm_units,vocab_size,batch_size=1)\n",
        "#model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.load_weights(checkpoint_filepath)\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-VAAhUpFxWb",
        "colab_type": "text"
      },
      "source": [
        "### Write Novel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JY-d4FLuR9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_string=\"段譽卻仍是抬起了頭望著她，見那少女雙腳蕩啊蕩的，似乎這麼坐樑上甚是好玩，問道：“是你救我的麼？\"\n",
        "\n",
        "print(generate_text(model,temperature,start_string,num_generate))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw0CiDRKhsFa",
        "colab_type": "text"
      },
      "source": [
        "# Measurements & Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXyQCGSuVqRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls ./logs/fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp_x_AfziXet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/fit/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}