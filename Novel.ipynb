{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Copy of Copy of Novel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s99436q/RNN/blob/master/Novel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BjXorqANZ8c",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYk9h4M0R_Z1",
        "colab_type": "code",
        "outputId": "6d5bacff-4b84-4744-9474-38ccbb30df23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "novel2.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6TgybT1YB5Y",
        "colab_type": "code",
        "outputId": "2ce41b41-40dc-4a39-ec4b-ce7b67600fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "e42zeqfnNZ8e",
        "colab_type": "code",
        "outputId": "b33d5330-3de6-48ac-a31d-01b1cf14b2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "text=open(\"novel2.txt\", encoding=\"utf-8\").read()\n",
        "print(f\"天龍八部小說共有 {len(text)} 中文字\")\n",
        "print(f\"包含了 {len(set(text))} 個獨一無二的字\")\n",
        "print()\n",
        "print(text[100031:100099])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "天龍八部小說共有 1266445 中文字\n",
            "包含了 4401 個獨一無二的字\n",
            "\n",
            "凶神惡煞’南海鱷神之上，必定是個狠惡可怖之極的人物，那知居然頗有姿色，不由得又向她瞧了幾眼。葉二孃向她嫣然一笑，木婉清全身一顫，只覺她這\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn15yfabNZ8r",
        "colab_type": "text"
      },
      "source": [
        "# Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwxdoNJJNZ8x",
        "colab_type": "code",
        "outputId": "e0fff2ce-a1bc-4257-f05a-0151d9861d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "# 初始化一個以字為單位的 Tokenizer\n",
        "#tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "#    num_words=None,char_level=True,filters=''\n",
        "#)\n",
        "    \n",
        "# tokenizer 看一遍天龍八部全文，將每個新出現的字加入字典並將中文字轉成對應的數字索引\n",
        "#tokenizer.fit_on_texts(text)\n",
        "#text_as_int = tokenizer.texts_to_sequences([text])[0] #return text=[[...]]\n",
        "\n",
        "words=set(text)\n",
        "char2index = {u:i for i, u in enumerate(words)}\n",
        "index2char = np.array(words)\n",
        "\n",
        "text_as_int = np.array([char2index[c] for c in text])\n",
        "index2char\n",
        "text_as_int"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 544, 2955, 1713, ..., 3515,  888, 1392])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjxchXLENZ83",
        "colab_type": "text"
      },
      "source": [
        "# Prepare training data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcp3TTryNZ86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQ_LENGTH = 100\n",
        "BATCH_SIZE = 128 \n",
        "steps_per_epoch = len(text_as_int) // SEQ_LENGTH\n",
        "\n",
        "# transfer text_as_int to tensor\n",
        "characters = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "# cut the novel to small pieces of seqs\n",
        "sequences = characters.batch(SEQ_LENGTH + 1,drop_remainder=True)\n",
        "\n",
        "# each seq to input/output pairs\n",
        "def build_seq_pairs(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# 1.文本擷取出來的序列套用上面定義的函式，拆成兩個數字序列作為輸入／輸出序列\n",
        "# 2.再將得到的所有數據隨機打亂順序\n",
        "# 3.最後再一次拿出 BATCH_SIZE（128）筆數據,作為模型一次訓練步驟的所使用的資料\n",
        "ds = sequences\\\n",
        "    .map(build_seq_pairs)\\\n",
        "    .shuffle(steps_per_epoch)\\\n",
        "    .batch(BATCH_SIZE, \n",
        "           drop_remainder=True)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCQck3zCNZ9B",
        "colab_type": "text"
      },
      "source": [
        "# Build the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "K4ZkLAB1NZ9C",
        "colab_type": "code",
        "outputId": "04bb8e21-341d-430c-a491-ffae7b079b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "# 超參數\n",
        "EMBEDDING_DIM = 512\n",
        "RNN_UNITS = 1024\n",
        "num_words=len(set(text))\n",
        "def build_model(EMBEDDING_DIM,RNN_UNITS,num_words,batch_size):\n",
        "      # 使用 keras 建立一個非常簡單的 LSTM 模型\n",
        "      model = tf.keras.Sequential()\n",
        "\n",
        "      # 詞嵌入層: 將每個索引數字對應到一個高維空間的向量\n",
        "      model.add(\n",
        "          tf.keras.layers.Embedding(\n",
        "              input_dim=num_words, \n",
        "              output_dim=EMBEDDING_DIM,\n",
        "              batch_input_shape=[\n",
        "                  batch_size, None]\n",
        "      ))\n",
        "\n",
        "      # LSTM 層:負責將序列數據依序讀入並做處理\n",
        "      model.add(\n",
        "          tf.keras.layers.LSTM(\n",
        "          units=RNN_UNITS, \n",
        "          return_sequences=True, \n",
        "          stateful=True, \n",
        "          recurrent_initializer='glorot_uniform'\n",
        "      ))\n",
        "\n",
        "      # 全連接層:負責model每個中文字出現的可能性\n",
        "      model.add(\n",
        "          tf.keras.layers.Dense(\n",
        "              num_words))\n",
        "\n",
        "      model.summary()\n",
        "      return model\n",
        "model=build_model(EMBEDDING_DIM,RNN_UNITS,num_words,BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 512)          2253312   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (128, None, 1024)         6295552   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 4401)         4511025   \n",
            "=================================================================\n",
            "Total params: 13,059,889\n",
            "Trainable params: 13,059,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4deo5Q9NZ9K",
        "colab_type": "text"
      },
      "source": [
        "# Set Model Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtHVrPMtNZ9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 超參數，決定模型一次要更新的步伐有多大\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# 定義模型預測結果跟正確解答之間的差異\n",
        "# 因為全連接層沒使用 activation func, from_logits= True \n",
        "def loss(y_true, y_pred):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        y_true, y_pred, from_logits=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(\n",
        "        learning_rate=LEARNING_RATE), \n",
        "    loss=loss\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utUCWEJuNZ9T",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI7WHkv5bNXj",
        "colab_type": "code",
        "outputId": "a7e875e8-3481-4537-a253-dd5168b69d6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3),\n",
        "]\n",
        "\n",
        "#!rm -rf ./logs/\n",
        "!rm -rf ./training_checkpoints\n",
        " \n",
        "EPOCHS=20\n",
        "history = model.fit(\n",
        "    ds,# 前面使用 tf.data 建構的資料集\n",
        "    epochs=EPOCHS, \n",
        "    callbacks=[callbacks]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "97/97 [==============================] - 30s 311ms/step - loss: 6.0825\n",
            "Epoch 2/20\n",
            " 5/97 [>.............................] - ETA: 22s - loss: 5.5114"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZfU1J8MNZ9b",
        "colab_type": "text"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3GGfj6-iJRr",
        "colab_type": "text"
      },
      "source": [
        "###Retore the lastes Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9xBYrH2iA7j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R9YY4zUjBOT",
        "colab_type": "text"
      },
      "source": [
        "###Build a new Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S92OGgv6jAie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_model=build_model(EMBEDDING_DIM, RNN_UNITS, num_words,batch_size=1)\n",
        "p_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "p_model.build(tf.TensorShape([1,None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjzy2OtqlRUH",
        "colab_type": "text"
      },
      "source": [
        "###Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcDm7wxblQdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(p_model,start_string):\n",
        "  num_generate=150\n",
        "  #convert string to nums (vectorizing)\n",
        "  input_val=[char2index[c] for c in start_string]\n",
        "  input_val=tf.expand_dims(input_val,0)\n",
        "  print(len(input_val[0]),len(start_string))\n",
        "  text_generated=[]\n",
        "  temperature=1.0\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    prediction=p_model(input_val)\n",
        "    prediction=tf.squeeze(prediction,0)\n",
        "    prediction/=temperature\n",
        "    \n",
        "    predict_id=tf.random.categorical(prediction,num_samples=1)[-1,0].numpy()\n",
        "    input_val=tf.expand_dims([predict_id],0) \n",
        "    text_generated+=[index2char[predict_id]]\n",
        "\n",
        "  return start_string+''.join(text_generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JY-d4FLuR9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "content=\"段譽卻仍是抬起了頭望著她，見那少女雙腳蕩啊蕩的，似乎這麼坐樑上甚是好玩，問道：“是你救我的麼？\"\n",
        "\n",
        "print(generate_text(p_model,start_string=content))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw0CiDRKhsFa",
        "colab_type": "text"
      },
      "source": [
        "# Measurements & Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXyQCGSuVqRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls ./logs/fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp_x_AfziXet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/fit/"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}